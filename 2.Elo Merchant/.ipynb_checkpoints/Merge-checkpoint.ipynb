{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ef20fd4e6471fd2c9e3022edfdddc07a970aca"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5e5037c5a33ef76cd69caf095db1f6320b91e5f"
   },
   "outputs": [],
   "source": [
    "print('tung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#this is used to reduce the memory usage of the dataframe \n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50e1f88df96b5e525237fe120650e679b7f60654"
   },
   "outputs": [],
   "source": [
    "#Read the csv \n",
    "new_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "df_merchant = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')\n",
    "def binarize(df):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    df : input dataframe \n",
    "    \n",
    "    Return \n",
    "    ----------\n",
    "    dataframe with column authorized_flag and category_1 ,mapped to either 1 or 0 \"\"\"\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    return df\n",
    "\n",
    "historical_transactions = binarize(historical_transactions)\n",
    "new_transactions = binarize(new_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a940a9ce940fa489a5a5255f6d4523c9b7c3ffdc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def read_data(input_file):\n",
    "    #change the column type to date type\n",
    "    #calculate the elapsed time \n",
    "    df = pd.read_csv(input_file)\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    return df\n",
    "\n",
    "train = read_data('../input/elo-merchant-category-recommendation/train.csv')\n",
    "test = read_data('../input/elo-merchant-category-recommendation/test.csv')\n",
    "\n",
    "#take out the target column in the training data \n",
    "target = train['target']\n",
    "del train['target']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd9f71f39664eef55e0b3ec3e6172e84a8e1a963"
   },
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "170f0e959ba5250b191b7cee6b586bdabd347018",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#create one-hot-encoding dataframe for category_2 and category_3\n",
    "historical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\n",
    "new_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n",
    "\n",
    "#reduce memory usage\n",
    "historical_transactions = reduce_mem_usage(historical_transactions)\n",
    "new_transactions = reduce_mem_usage(new_transactions)\n",
    "df_merchant=reduce_mem_usage(df_merchant)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b37dfa25e803f3bdbc8125221b5e01c43b4d2f61"
   },
   "outputs": [],
   "source": [
    "df_merchant['abs_numerical_1'] = abs(df_merchant['numerical_1'])+1\n",
    "cols = ['merchant_id','abs_numerical_1']\n",
    "df_trick = df_merchant[cols]\n",
    "df_trick.head()\n",
    "\n",
    "\n",
    "#     historical_transactions['today_date'] = pd.to_datetime('2018-2-1')\n",
    "#     historical_transactions['day_diff'] = (historical_transactions['today_date'] - historical_transactions['purchase_date']).dt.days\n",
    "#     historical_transactions = historical_transactions(right = df_trick , how='left', on = 'merchant_id')\n",
    "#     historical_transactions['2w_purchase'] = np.where(historical_transactions['day_diff'] <= 14,historical_transactions['purchase_amount'],0)\n",
    "#     historical_transactions['1m_purchase'] = np.where(historical_transactions['day_diff'] <= 30,historical_transactions['purchase_amount'],0)\n",
    "#     historical_transactions['3m_purchase'] = np.where(historical_transactions['day_diff'] <= 90,historical_transactions['purchase_amount'],0)\n",
    "#     historical_transactions['6m_purchase'] = np.where(historical_transactions['day_diff'] <= 180,historical_transactions['purchase_amount'],0)\n",
    "#     historical_transactions['1y_purchase'] = np.where(historical_transactions['day_diff'] <= 365,historical_transactions['purchase_amount'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5ea91620e9a8f2835ade875eb82e626ee185c6b"
   },
   "outputs": [],
   "source": [
    "historical_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51a4f5316be60df2d8904f41b3e76b5742c5efad"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Amount of purchase in the last month\n",
    "# for df in [historical_transactions,new_transactions]:\n",
    "def purchase_lapse(df):\n",
    "    df['today_date'] = pd.to_datetime('2018-2-1')\n",
    "    df['day_diff'] = (df['today_date'] - df['purchase_date']).dt.days\n",
    "    df = df.merge(right = df_trick , how='left', on = 'merchant_id')\n",
    "    df['purchase_amount'] = df['purchase_amount']*df['abs_numerical_1']\n",
    "    df['2w_purchase'] = np.where(df['day_diff'] <= 14,df['purchase_amount'],0)\n",
    "    df['1m_purchase'] = np.where(df['day_diff'] <= 30,df['purchase_amount'],0)\n",
    "    df['3m_purchase'] = np.where(df['day_diff'] <= 90,df['purchase_amount'],0)\n",
    "    df['6m_purchase'] = np.where(df['day_diff'] <= 180,df['purchase_amount'],0)\n",
    "    df['1y_purchase'] = np.where(df['day_diff'] <= 365,df['purchase_amount'],0)\n",
    "    return df\n",
    "\n",
    "historical_transactions = purchase_lapse(historical_transactions)\n",
    "new_transactions = purchase_lapse(new_transactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "726744b2e916d52e08517411c831e77db96aaf7e"
   },
   "outputs": [],
   "source": [
    "agg_fun = {'authorized_flag': ['sum', 'mean']}\n",
    "\n",
    "#groupby ohe dataframe by card_id\n",
    "auth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\n",
    "auth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\n",
    "auth_mean.reset_index(inplace=True)\n",
    "\n",
    "authorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\n",
    "historical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c3cec859b8f5eed1a384c6f464fe0498f841420"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#create purchase month\n",
    "historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\n",
    "authorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\n",
    "new_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5de9c3bcc2ad2dfeb943e1b02fef3e433c91af3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggregate_transactions(history):\n",
    "    '''\n",
    "    Parameters \n",
    "    -----------\n",
    "    history: historical transaction of dataframe\n",
    "    \n",
    "    \n",
    "    output:\n",
    "    ----------\n",
    "    dataframe groupby card_id and aggregated on multiple function\n",
    "    '''\n",
    "    \n",
    "    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean'],\n",
    "        'category_2_2.0': ['mean'],\n",
    "        'category_2_3.0': ['mean'],\n",
    "        'category_2_4.0': ['mean'],\n",
    "        'category_2_5.0': ['mean'],\n",
    "        'category_3_A': ['mean'],\n",
    "        'category_3_B': ['mean'],\n",
    "        'category_3_C': ['mean'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        '2w_purchase':['sum','mean','max','min','std'],\n",
    "        '1m_purchase':['sum','mean','max','min','std'],\n",
    "        '3m_purchase':['sum','mean','max','min','std'],\n",
    "        '6m_purchase':['sum','mean','max','min','std'],\n",
    "        '1y_purchase':['sum','mean','max','min','std'],\n",
    "#         'merchant_category_id': ['nunique'],\n",
    "#         'state_id': ['nunique'],\n",
    "#         'city_id': ['nunique'],\n",
    "#         'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum',\n",
    "                            'mean',\n",
    "                            'max',\n",
    "                            'min',\n",
    "                            'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std'],\n",
    "        'purchase_date': [np.ptp, 'min', 'max'],\n",
    "        'month_lag': ['min', 'max','std','sum',]\n",
    "        }\n",
    "    #groupby card id \n",
    "    agg_history = history.groupby(['card_id']).agg(agg_func)\n",
    "    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n",
    "    agg_history.reset_index(inplace=True)\n",
    "    \n",
    "    df = (history.groupby('card_id')\n",
    "          .size()\n",
    "          .reset_index(name='transactions_count'))\n",
    "    \n",
    "    #merge the two dataframe together \n",
    "    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n",
    "    \n",
    "    return agg_history\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf1f963f61841ef34f79fd16364bbf59b7f4e8a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = aggregate_transactions(historical_transactions)\n",
    "history.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\n",
    "gc.collect()\n",
    "history[:5].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f07e2c3f259947d21072e765737acfd616ee2315"
   },
   "outputs": [],
   "source": [
    "history[:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c1bdbaa7c16e22436893354b31d47e1846330ab"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#authorized transaction\n",
    "authorized = aggregate_transactions(authorized_transactions)\n",
    "authorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e289a69edfb2883c92309d636869f508e90c5829"
   },
   "outputs": [],
   "source": [
    "authorized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cc2775c29c17cb7438f3f5c45777b894afa94fc"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#doing the same thing for the new transaction\n",
    "new = aggregate_transactions(new_transactions)\n",
    "new.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "494e66efb796eefc9e860b0a335f095d3b9ae68a"
   },
   "outputs": [],
   "source": [
    "new[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb443fb1e2a47f5a0bef02921a2cba1d4e95e673"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def aggregate_per_month(history):\n",
    "    '''\n",
    "    Parameter:\n",
    "    ----------\n",
    "    history transaction dataframe\n",
    "    \n",
    "    Output:\n",
    "    ---------\n",
    "    Create a new dataframe aggregated per month\n",
    "    order : groupby card_id + month_lag ----> groupby card_id \n",
    "    '''\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n",
    "            }\n",
    "    #group by and create new column\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "    \n",
    "    #secondary group by card_id\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group\n",
    "#___________________________________________________________\n",
    "final_group =  aggregate_per_month(historical_transactions) \n",
    "final_group[:10]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e5e313f605160783127a6f0798f176eaf6a55a4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#merge all the dataframe together\n",
    "train = pd.merge(train, history, on='card_id', how='left')\n",
    "test = pd.merge(test, history, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, authorized, on='card_id', how='left')\n",
    "test = pd.merge(test, authorized, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, new, on='card_id', how='left')\n",
    "test = pd.merge(test, new, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, final_group, on='card_id', how='left')\n",
    "test = pd.merge(test, final_group, on='card_id', how='left')\n",
    "\n",
    "train = pd.merge(train, auth_mean, on='card_id', how='left')\n",
    "test = pd.merge(test, auth_mean, on='card_id', how='left')\n",
    "\n",
    "print(\"Train Shape:\", train.shape)\n",
    "print(\"Test Shape:\", test.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "801ef73e8aff72b8a1fbca98206015ff29dc464c"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad92c9ad33dcf8fe89fb5bb0310881f6e599273e"
   },
   "outputs": [],
   "source": [
    "# train.dropna(inplace = True)\n",
    "# test.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1d36f8097ecfbd910827ae2d60d91dc74a53f61"
   },
   "outputs": [],
   "source": [
    "#exclude the card_id and first_active month in the features \n",
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "#create a list of categorical features \n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fb7cc23fb41d48dd9f0de4aa1e72f09248f3d75"
   },
   "outputs": [],
   "source": [
    "#perform standardscaler for the train and test dataset \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "train_scaled = sc.fit_transform(train[features])\n",
    "test_scaled = sc.fit_transform(test[features])\n",
    "train_df = pd.DataFrame(train_scaled,columns = train[features].columns)\n",
    "test_df = pd.DataFrame(test_scaled,columns = test[features].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "796d4e17b4c4cc3121fc833669fc2eaf07bf4fba"
   },
   "outputs": [],
   "source": [
    "train_df['card_id'] = train['card_id']\n",
    "test_df['card_id'] = test['card_id']\n",
    "train_df['first_active_month'] = train['first_active_month']\n",
    "test_df['first_active_month'] = test['first_active_month']\n",
    "\n",
    "train = train_df\n",
    "test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0aff6b94b8b6383fd20156c381c9b1a37e599405"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b926c1b3a703d8ed6c7c609e7c892b503e3c511"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28ac4d7abc8117671e8e79caaabcbf41dbb328c0"
   },
   "outputs": [],
   "source": [
    "#lightgbm configuration\n",
    "param = {'num_leaves': 50,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.005,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"nthread\": 4,\n",
    "         \"verbosity\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92bf1d3418327cb759937b8f32f40d75e73108e9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=10, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(train))\n",
    "predictions = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 100000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 300)\n",
    "    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98eb235d712c857ebc3dc3fff312e049c811267a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acdaccc5c862736b097ae9e5bcdcdae5083b5d33"
   },
   "source": [
    "## LightGBM-1 with Repeated kfold approach\n",
    "\n",
    "#### RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91043a8b0a7b1d335672f35d39f1621b02befc88"
   },
   "outputs": [],
   "source": [
    "lgbparam = {'num_leaves': 31,\n",
    "#             'boosting_type': 'rf',\n",
    "             'min_data_in_leaf': 30, \n",
    "             'objective':'regression',\n",
    "             'max_depth': 12,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1,\n",
    "             \"nthread\": 4,\n",
    "             \"random_state\": 4590}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17b17b5d967ccf2912d9f7b229cd06cb74819d85",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "folds = RepeatedKFold(n_splits=10, n_repeats=2, random_state=4520)\n",
    "\n",
    "oof_lgb = np.zeros(len(train))\n",
    "predictions_lgb = np.zeros(len(test))\n",
    "start = time.time()\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 100000\n",
    "    clf = lgb.train(lgbparam, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=3000, early_stopping_rounds = 300)\n",
    "    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgb += clf.predict(test[features], num_iteration=clf.best_iteration) / (5 * 2)\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof_lgb, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b84242fc4c57f96b36b860d5728cddaf3b8c2943"
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0362877a4fb45006383dc3bc03b5642aba0f47dc"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"submit_lgb.csv\", index=False)\n",
    "\n",
    "sub_df1 = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\n",
    "sub_df1[\"target\"] = predictions_lgb\n",
    "sub_df1.to_csv(\"submit_lgb1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dfa7a5fcabd78829678fa4283e99862f1c4a0d7"
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c9afce8b59b5f51024150a997a9d2eb702ab85d"
   },
   "outputs": [],
   "source": [
    "train_stack = np.vstack([oof,oof_lgb]).transpose()\n",
    "test_stack = np.vstack([predictions,predictions_lgb]).transpose()\n",
    "\n",
    "folds = RepeatedKFold(n_splits=5,n_repeats=1,random_state=4520)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_stack = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "    print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "#     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
    "#     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n",
    "    clf = BayesianRidge()\n",
    "    clf.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = clf.predict(val_data)\n",
    "    predictions_stack += clf.predict(test_stack) / 5\n",
    "\n",
    "\n",
    "np.sqrt(mean_squared_error(target.values, oof_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e871862de58a3ce43c148f78f152c1d6385a0a08"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/elo-merchant-category-recommendation/sample_submission.csv')\n",
    "sample_submission['target'] = predictions_stack\n",
    "sample_submission.to_csv('Bayesian_Ridge_Stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b65e47686b49b31f0dc47171ab2aece4a4d0a941"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/elo-merchant-category-recommendation/sample_submission.csv')\n",
    "sample1 = pd.read_csv(\"../input/elo-blending/3.695.csv\")\n",
    "sample2 = pd.read_csv(\"../input/elo-blending/combining_submission (1).csv\")\n",
    "sample_submission['target'] = predictions * 0.5 + predictions_lgb * 0.5\n",
    "sample_submission.to_csv(\"Blend1.csv\", index = False)\n",
    "sample_submission['target'] = sample_submission['target'] * 0.2 + sample1['target'] * 0.2 + sample2['target'] * 0.6\n",
    "sample_submission.to_csv('Blend2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
